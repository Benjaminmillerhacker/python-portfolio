# -*- coding: utf-8 -*-
"""NLP Parts-of-Speech tagging - NLTK spaCy Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13mzV74w8j-cWP2yaDn4RpEZLlVbFb2tz

## Parts of Speech Tagging 

### Comparison of Methods

Let's compare different Parts-Of-Speech (POS) taggers. Python has some popular libraries for Natural Language Processing:

- NLTK

- spaCy

- fastText

- gensim

Using the Treebank tagged corpus in the NLTK library, we'll compare the accuracy for some of NLTK's and spaCy's taggers, as well as a simple bidirectional LSTM trained using Keras, with a Tensorflow backend on a Google Colab instance.

### Libraries
"""

import random
import numpy as np
import pandas as pd
from collections import Counter, defaultdict

from keras import backend as K
from keras.engine import Layer
from keras.layers import (Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, 
                          Embedding, Activation)
from keras.models import Sequential
from keras.optimizers import Adam
from keras.preprocessing.sequence import pad_sequences
from keras.utils import multi_gpu_model

import nltk
from nltk import ConfusionMatrix
from nltk.corpus import treebank, treebank_chunk
from nltk.tag import (UnigramTagger, BigramTagger, RegexpTagger, DefaultTagger,  
                      PerceptronTagger)

from sklearn.model_selection import train_test_split

import spacy

import tensorflow as tf
import tensorflow_hub as hub

from utils_pos_tagging import (compare_taggers, get_tag_list, apply_tagger, get_performance_dataframe,
                               get_spacy_test_sentences, get_spacy_accuracy, flatten_tagged_sentences,
                               get_word2index, get_tag2index, sentence2int, tag2int, one_hot_encoding)

from IPython.display import display

nltk.download('tagsets')
nltk.download('treebank')
nltk.download('maxent_treebank_pos_tagger')
nltk.download('averaged_perceptron_tagger')

"""### Penn Treebank Documentation

>[ PENN TREEBANK SAMPLE ]
http://www.cis.upenn.edu/~treebank/home.html

>This is a ~5% fragment of Penn Treebank, (C) LDC 1995.  It is made
available under fair use for the purposes of illustrating NLTK tools
for tokenizing, tagging, chunking and parsing.  This data is for
non-commercial use only.
"""

tagged_sentences = treebank.tagged_sents()
print(random.choice(tagged_sentences))
print("Tagged sentences:", len(tagged_sentences))
print("Tagged words:", len(treebank.tagged_words()))

# tag raw count
tag_list = list()
for s in tagged_sentences:
    tag_list.extend([i[1] for i in s])
pd.Series(Counter(tag_list)).sort_values(ascending=False).head(10)

# visualisation
display(treebank_chunk.chunked_sents()[0])

"""## NLTK taggers

The following built in taggers are tested:

- `RegexpTagger`

- `DefaultTagger`

- `UnigramTagger`

- `BigramTagger`

- `PerceptronTagger`
"""

# Keeping it simple for now, 80:20 train/test split without random sampling
split = int(len(tagged_sentences)*0.2)
train_sentences = tagged_sentences[split:]
test_sentences = tagged_sentences[:split]

# some regex patterns from NLTK documentation
patterns = [
    (r'.*ing$', 'VBG'),               # gerunds
    (r'.*ed$', 'VBD'),                # simple past
    (r'.*es$', 'VBZ'),                # 3rd singular present
    (r'.*ould$', 'MD'),               # modals
    (r'.*s$', 'NNS'),                 # plural nouns
    (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers
    (r'.*able$', 'JJ'),               # adjectives
    (r'.*ly$', 'RB'),                 # adverbs
    (r'.*', 'NN')                     # nouns
]

# Commented out IPython magic to ensure Python compatibility.
# %timeit regexp_tagger = RegexpTagger(patterns)

# Commented out IPython magic to ensure Python compatibility.
# %timeit default_tagger = DefaultTagger('NN') # majority tag by inspection above

# Commented out IPython magic to ensure Python compatibility.
# %timeit unigram_tagger = UnigramTagger(train_sentences)

# Commented out IPython magic to ensure Python compatibility.
# %timeit bigram_tagger = BigramTagger(train_sentences)

# Commented out IPython magic to ensure Python compatibility.
# %%timeit -r 3
# perceptron_tagger = PerceptronTagger()
# perceptron_tagger.train(train_sentences)

# training set only
regexp_tagger = RegexpTagger(patterns)
default_tagger = DefaultTagger('NN') # majority tag by inspection above
unigram_tagger = UnigramTagger(train_sentences)
bigram_tagger = BigramTagger(train_sentences)
perceptron_tagger = PerceptronTagger()
perceptron_tagger.train(train_sentences)

"""## Test set evaluation of NLTK taggers 

### Accuracy
"""

tagger_dict = {
    'RegExpTagger': regexp_tagger,
    'DefaultTagger': default_tagger,
    'UnigramTagger': unigram_tagger,
    'BigramTagger': bigram_tagger,
    'PerceptronTagger': perceptron_tagger
}
compare_df = compare_taggers(tagger_dict, test_sentences)

compare_df.sort_values(['Accuracy', 'F1'], ascending=False)

"""PerceptronTagger **per-token accuracy** on the test set at 0.966 approaches the generally reported 97% figures [1] for state of the art machine learning POS tagging. It may be possible to push the metric further up if the entire Treebank corpus were available to expand the training set, instead of the current ~5% subset, but with diminishing returns of performance gain.

It is important to note that a per-token accuracy metric does not reflect adequately the quality of tagging on a sentence level. [2]

[1] https://aclweb.org/aclwiki/POS_Tagging_(State_of_the_art)

[2] https://nlp.stanford.edu/pubs/CICLing2011-manning-tagging.pdf

### Confusion Matrix
"""

# Picking the best performing PerceptronTagger as example
truth = get_tag_list(test_sentences)
preds = get_tag_list(apply_tagger(perceptron_tagger, test_sentences))
confusion_matrix = ConfusionMatrix(truth, preds)

print(confusion_matrix.pretty_format(show_percents=True, truncate=10, sort_by_count=True))

"""We can also look at the individual metrics for each tag to give a finer grained picture over the weighted average metrics for Precision, Recall and F1-Score calculated above."""

test_tag_list = list()
for s in test_sentences:
    test_tag_list.extend([(i[0], i[1]) for i in s])

# again using the perception tagger as an example
# precision, recall, f1 scores for each individual class
print('perceptron_tagger')
get_performance_dataframe(perceptron_tagger, test_tag_list)

"""## spaCy tagger: pretrained ConvNet

Loading up `en_core_web_sm` and `en_core_web_md` models for spaCy to use for the POS tagger. 

From documentation about these models:

> English multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.

> OntoNotes Release 5.0 is the final release of the OntoNotes project, a collaborative effort between BBN Technologies, the University of Colorado, the University of Pennsylvania and the University of Southern Californias Information Sciences Institute. The goal of the project was to annotate a large corpus comprising various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic) with structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference).

> Common Crawl's web archive consists of petabytes of data collected since 2011. It completes crawls generally every month.
"""

# small model
sm_nlp = spacy.load('en_core_web_sm')
# medium model
md_nlp = spacy.load('en_core_web_md')

# inspecting top tags using small model
sm_doc = sm_nlp(' '.join([i[0] for i in test_tag_list]))
pd.Series(Counter([token.tag_ for token in sm_doc])).sort_values(ascending=False).head(10)

"""spaCy's pipeline takes care of tokenisation and parsing. However, the results of its tokenisation are different from that of NLTK's, so to arrive at an accuracy figure we can't do a straightforward plug-in to the evaluation functions written for NLTK's taggers."""

print('Number of elements from NLTK tokenisation:', len(test_tag_list)) 
print('Number of elements from spaCy:', len([(token.text, token.tag_) for token in sm_doc]))

spacy_test_sentences = get_spacy_test_sentences(test_sentences)

"""### Accuracy for spaCy tagger"""

print("small model accuracy:", get_spacy_accuracy(sm_nlp, spacy_test_sentences))
print("medium model accuracy:", get_spacy_accuracy(md_nlp, spacy_test_sentences))

"""At first glance, it looks as though the accuracy score for spaCy's pretrained small and medium ConvNet English POS tagger models are both poorer than the PerceptronTagger of NLTK when evaluated on the same test sentences.

**However** this should not immediately be interpreted as the spaCy tagger having poorer performance, because there is a difference in the set of Treebank tags by NLTK and spaCy:
"""

# set difference of NLTK Treebank tags and spaCy Treebank tags
set(tag_list) - set([token.tag_ for token in sm_doc])

"""For this task, since the gold reference of tags is according to NLTK, spaCy's tagging will necessarily be judged 'incorrect' whenever these 4 tags are applied. Therefore no conclusion should be drawn as yet to the relative performances of NLTK and spaCy's taggers; more work needs to be done to ensure any comparison is made on an even footing.

## Bidirectional LSTM

No transfer learning applied for now. A logical next step improve the model would be to put in a pretrained Elmo embedding layer which would reduce the number of parameters that need to be trained, and only a small number left trainable for fine tuning. Some other transfer learning method using Bert / ULMFiT would also be viable.

###  Data Munging
"""

# tagged_sentences = treebank.tagged_sents()
sentences, sentence_tags = flatten_tagged_sentences(tagged_sentences)
# 80:20 split
train_sentences, test_sentences, train_tags, test_tags = \
                train_test_split(sentences, sentence_tags, test_size=0.2, random_state=42)

# convert to integers
word2index = get_word2index(train_sentences)
tag2index = get_tag2index(train_tags)
train_sentences_X = sentence2int(train_sentences, word2index)
test_sentences_X = sentence2int(test_sentences, word2index)
train_tags_y = tag2int(train_tags, tag2index)
test_tags_y = tag2int(test_tags, tag2index)

# Pad sequences to maximum length of training sentences to give fixed size
MAX_LENGTH = len(max(train_sentences_X, key=len))
train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')
test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')
train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')
test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')

train_tags_y_categorical = one_hot_encoding(train_tags_y, len(tag2index))
test_tags_y_categorical = one_hot_encoding(test_tags_y, len(tag2index))

"""### Model building

Let's build a bidirectional LSTM, no hyperparameter tuning and no transfer learning.

Can you spot if this model is appropriate for the task at hand?
"""

def build_model():
    model = Sequential()
    model.add(InputLayer(input_shape=(MAX_LENGTH, )))
    model.add(Embedding(len(word2index), 64))
    model.add(Bidirectional(LSTM(128, return_sequences=True)))
    model.add(TimeDistributed(Dense(len(tag2index))))
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy',
                  optimizer=Adam(lr=0.001),
                  metrics=['accuracy'])

    model.summary()
    return model

model = build_model()
model.fit(train_sentences_X, 
          train_tags_y_categorical,
          batch_size=64, epochs=20, validation_split=0.2)

# model.save('colab_trained_lstm.h5')

scores = model.evaluate(test_sentences_X, 
                        test_tags_y_categorical)
print("{}: {}".format(model.metrics_names[1], scores[1]))

### Discussion

The above model was trained on a GPU instance with Google Colab. The exceptionally high accuracy score beyond the reported state of the art would suggest overfitting, even though the performance evaluation is done on an unseen set.

This can readily be explained by the size of the dataset:

- Tagged sentences: 3914

- Tagged words: 100676

which is very small, versus the massive learning capacity of the bidirectional LSTM with 197632 parameters.

As a general point, some measures can be taken to minimise the chances of overfitting:

- specifying a smaller network to reduce capacity of learning

- weight regularisation

- dropout

- early stopping