# -*- coding: utf-8 -*-
"""Titanic Survival.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xATHYuE2vtqN_uMTpLrgwmXgqbyjWq8q

# Modelling Titanic Survival
## Using k-Nearest Neighbours Classification, Model Evaluation, PostgreSQL

For this project we showcase several things:

- Loading data from an AWS PostgreSQL instance
- Create a logistic regression model to figure out the likelihood of a passenger's survival
- Use the supervised machine learning model k-Nearest Neighbours for classification
- Hyperparameter optimisation through GridSearchCV
- Model Evaluation through confusion matrices and ROC curves

## Problem statement.

The aim here is to train predictive models on the 1912 Titanic disaster dataset. The performance of these models will be judged by appropriate scoring and comparison with a suitable baseline accuracy.

## Goal.

Based on the 1912 Titanic disaster dataset, train and cross validate a model using suitable predictors to predict passenger survival.

Now let's import the libraries that we will be using. `sklearn` features heavily in this project, as will it for many other projects that involve machine learning. It allows the Machine Learning practitioner to quickly apply many different models without getting too involved into the details of implementation, **but we mustn't let it become a black box!** The sci-kit learn documentation is very well written and provides a handy reference, but the fundamental understanding that is bread and butter of a Data Scientist can only be acquired through study. The book _Introduction to Statistical Learning_ is an amazing resource that goes through the vast majority of Machine Learning models, using `R` code as examples. Feel free to ask me about the details of the models I use!
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
#from patsy import dmatrices
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, auc
from sklearn.neighbors import KNeighborsClassifier
# %matplotlib inline

"""## Data Acquisition.

#### 1. Connecting to the remote database

There are a multitude of ways to connect to a SQL database, one of which that can be easily applied to a Jupyter Notebook is __cell magic__ using `%sql`. Another method is the library `sqlalchemy` which we will see more of in other projects.

psql -h dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com -p 5432 -U dsi_student titanic
password: ########
# edited out password
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext sql

# Commented out IPython magic to ensure Python compatibility.
# %sql postgresql://dsi_student:#######@dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com/titanic

"""#### 2. Querying the database and aggregating the data"""

# Commented out IPython magic to ensure Python compatibility.
# %%sql
# SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_type = 'BASE TABLE';

# Commented out IPython magic to ensure Python compatibility.
# %%sql
# SELECT * FROM train LIMIT 2;

"""### Risks & Assumptions

#### Risks

Although information for cabin number was given for some passengers, it was excluded from this analysis since we have no map or chart to corroborate this information with a location. It is within reason to presume that the location of the passengers' cabins may be correlated with their chances of survival, but this factor is left out of this analysis.

#### Assumptions

The reason why the variables Parch and SibSp are included is that people with larger families may presumably have affect one's chance of survival, which would be reasonable given the instinct to protect one's kin.

## Exploratory Data Analysis

#### 1. Describing the Data
"""

import sqlalchemy
engine = sqlalchemy.create_engine("postgresql://dsi_student:gastudents@dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com/titanic")

pd.read_sql("SELECT COUNT(index) FROM train;",con=engine)

"""The dataset is of a reasonable size and can be read in its entirety into a Pandas DataFrame."""

dataset = pd.read_sql("SELECT * FROM train;",con=engine)
print dataset.shape
dataset.head(2)

"""`pandas` DataFrames come with a handy function `describe()`, which conveniently wraps up in one table key summary statistics of any quantitative features:

- Number of non-null entries per feature in the dataset
- Mean, Standard Deviation
- Minimum, Maximum
- 25%, 50%, 75% Quartiles
"""

dataset.describe()

"""#### 2. Visualising the Data

Let's use our handy `heatmap()` method from `seaborn` to visualise correlations. It immediately jumps out at us that the features 'Fare' and 'Pclass' have the strongest correlations with survival, though in opposite senses; we further observe that these two features themselves are strongly negatively correlated, which makes perfect sense!

So through this one visualisation we can already surmise that passenger class is an important feature for survival odds.
"""

import seaborn as sns
sns.heatmap(dataset.corr())

"""Doing some data cleaning:"""

dataset.loc[dataset["Age"].isnull()==True,"Age"] = dataset.loc[:,"Age"].median()

dataset.loc[dataset["Embarked"].isnull()==True,"Embarked"] = "S"

"""Once more we call on our handy `pairplot()` function from `seaborn` to visualise pair-wise relations between our features. Due to the fact that many of our features are categorical, certain plots can be difficult to interpret."""

cols = ["Survived","Pclass","Sex","Age","SibSp","Parch","Fare","Embarked"]
sns.pairplot(dataset[cols])

"""## Part 3: Data Wrangling

#### 1. Creating Dummy Variables for *Gender*
"""

df_ = dataset

dummies_sex = pd.get_dummies(df_.loc[:,["Sex"]],drop_first=True)

df_ = pd.concat([df_, dummies_sex], axis=1)
del df_["Sex"]
df_.head(2)

"""#### 2. Creating Dummy Variables for *Pclass*"""

dummies_pclass = pd.get_dummies(df_["Pclass"],drop_first=True, prefix="Pclass")

df_ = pd.concat([df_, dummies_pclass], axis=1)
del df_["Pclass"]
df_.head(2)

"""#### 3. Creating Dummy Variables for *Embarked*"""

dummies_embarked = pd.get_dummies(df_["Embarked"],drop_first=True, prefix="Embarked")

df_ = pd.concat([df_, dummies_embarked], axis=1)
del df_["Embarked"]
df_.head(2)

"""## Part 4: Logistic Regression and Model Validation

#### 1. Defining the predictor and target variables for classification
"""

features = ["Age","SibSp","Parch","Fare","Sex_male","Pclass_2","Pclass_3","Embarked_Q","Embarked_S"]
X = df_[features]
y = df_["Survived"]

"""#### 2. Logistic regression"""

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size=0.3,
                                                    stratify=y,
                                                    random_state=42)

logit = LogisticRegression(penalty='l2',C=1.0,fit_intercept=True)
model = logit.fit(X_train, y_train)

print "The training set accuracy is",model.score(X_train, y_train)

"""#### 3. Examining feature coefficients to see our correlations"""

pd.DataFrame(model.coef_, index=["Coefficients"], columns=X.columns)

"""#### 4. Testing the Model by introducing a *Test* or *Validation* set"""

scores = cross_val_score(model, X_test, y_test, cv=5)
scores

"""#### 5. Predicting the class labels for the *Test* set"""

y_pred = model.predict(X_test)
df_y_pred = pd.DataFrame(y_pred)
print "Model predicted labels for survival:\n",df_y_pred[0].value_counts(),"\n"
print "Actual labels in test set:\n",y_test.value_counts()

"""#### 6. Predict the class probabilities for the *Test* set"""

test_prob = model.predict_proba(X_test)
test_prob[0:3]

"""#### 7. Cross validation on the test set"""

print "The mean accuracy of 5-fold cross validation on the test set is", scores.mean(),","
print "with a standard deviation of", scores.std(),"."
print "\nThe training set accuracy is",model.score(X_train, y_train),"."

"""There is a small amount of overfitting, with a drop of 0.02 in accuracy score from the test set to the training set. This is in line with expectations.

#### 8. Checking the Classification Report
"""

print classification_report(y_test, y_pred)

"""### What do the classification metrics tell us?

Precision is the ratio TP / (TP + FP) where TP = number of true positives, FP = number of false positives. The precision measures the fitted model's ability to not falsely label as positive a sample that is negative.

Recall is the ratio TP / (TP + FN) where TP = number of true positives, FN = number of false negatives. The recall measures the model's ability to find all the positive samples.

F1-score is the harmonic mean of precision and recall.

The support is the number of occurrences of each class in y_test.

All metrics indicate that the fitted Logistic Regression model has a predictive ability exceeding random chance.

### Confusion Matrix
"""

# The scikit-learn docs provide us a handy example of code for creating a confusion matrix
import itertools

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_pred)
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=["Did not survive","Survived"],
                      title='Confusion matrix, without normalisation')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=["Did not survive","Survived"], normalize=True,
                      title='Normalised confusion matrix')

plt.show()

"""### What does the Confusion Matrix tell us?

The confusion matrix shows us the number of occurrences of True Positives, False Positives, True Negatives and False Negatives predicted by the fitted Logistic Regression model on the test set X\_test compared with the actual values y\_test.

### Plotting the ROC curve.
"""

# from sklearn docs
def auc_plotting_function(rate1, rate2, rate1_name, rate2_name, curve_name):
    AUC = auc(rate1, rate2)
    plt.plot(rate1, rate2, label=curve_name + ' (area = %0.2f)' % AUC, linewidth=3, c="c")
    plt.plot([0, 1], [0, 1], 'k--', linewidth=3)
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel(rate1_name, fontsize=18)
    plt.ylabel(rate2_name, fontsize=18)
    plt.title(curve_name, fontsize=18)
    plt.legend(loc="lower right")
    plt.show()

def plot_roc(y_true, y_score, title):
    fpr, tpr, _ = roc_curve(y_true, y_score)
    auc_plotting_function(fpr, tpr, 'False Positive Rate (1-specificity)', 'True Positive Rate (sensitivity)', 'ROC for '+title)

y_score = model.decision_function(X_test)
plot_roc(y_test, y_score, "Titanic passenger survival")

"""### What does the ROC curve tell us?

The Receiver Operating Characteristic curve is a plot of the true positive rate against the false positive rate for the different possible configurations of the model.

The ROC curve shows several things:

- It shows the tradeoff between sensitivity and specificity. Increases in sensitivity will be accompanied by a decrease in specificity.
- The closer the curve follows the left-hand border and then the top border of the ROC chart, the more accurate the test.
- The closer the curve comes to the 45 degree diagonal of the ROC space, the less accurate the test. 

In this case the ROC curve tells us the fitted model is reasonably accurate with a significantly higher AUC than the 45 degree diagonal.

## Gridsearch.

#### 1. Using GridSearchCV with logistic regression to search for optimal parameters 

We will be using 5-fold cross-validation, using the parameter grid shown in the cell below.
"""

logreg_parameters = {
    'penalty':['l1','l2'],
    'C':np.logspace(-5,1,50),
    'solver':['liblinear']
}

clf = GridSearchCV(model, param_grid = logreg_parameters, cv=5)
clf.fit(X_test, y_test)

"""### Best parameters and Best score. 
#### Are they better than the vanilla logistic regression?
"""

clf.best_params_

print "The best score from GridSearchCV is",clf.best_score_

model.get_params

"""For the vanilla logistic regression with _C = 1.0_ and _penalty = 'l2'_, the training set accuracy is 0.805778491172. The accuracy improvement from the best model using GridSearchCV is negligible.

### What is the difference in effect between the L1 (Lasso) and L2 (Ridge) penalties on the model coefficients?

Using L1 penalty during model training achieves feature selection at sufficiently high degrees of regularisation as controlled by the hyperparameter C in an inverse manner; the lower the C, the higher the degree of regularisation, and vice versa. Under such circumstances, some model coefficients are reduced to zero, and thus some features are eliminated from the final fitted model.

By contrast, L2 penalty does not result in feature selection. All feature coefficients are subject to minimisation from L2 regularisation, with the predictors that are less correlated with the target having lower absolute value coefficients, but they do not diminish to zero.

### What hypothetical situations are the Ridge and Lasso penalties useful?

Lasso penalty can be useful when there are a large number of features under consideration. By using feature selection, the fitted model with a smaller number of features can be easier to interpret. We may also choose Lasso penalty when domain expertise tells us that a number of features are not important and we wish to eliminate them.

By contrast, Ridge penalty can be chosen when there is uncertainty about the relative importance of predictors and when we do not want to rule out the influences of one predictor or another regardless of the magnitude of correlation.

## Gridsearch and kNN.

### Gridsearch on KNeighborsClassifier as the estimator

`GridSearchCV()` is a general method that can be applied to any Machine Learning model that has hyperparameters that can be tuned!
"""

knn = KNeighborsClassifier()

logreg_parameters = {
    'n_neighbors':[3,4,5,6,7,8,9,10],
    'weights':['uniform','distance'],
    'algorithm':['ball_tree','kd_tree','brute'],
    'p':[1,2]
}

clf2 = GridSearchCV(knn, param_grid=logreg_parameters, cv=5)
clf2.fit(X_test, y_test)

"""### Best parameters and score for the gridsearched kNN model."""

clf2.best_params_

clf2.best_score_

"""The score is poorer than that of the logistic regression model.

### In what hypothetical scenario might logistic regression be preferred over kNN, aside from model performance metrics?

Logistic regressions predict probabilities, which are a measure of the confidence of prediction. k-nearest neighbors predicts just the labels. We may choose logistic regression if we wish to preserve this measure of confidence of prediction, which may be revealing when the probabilities are close to 50% for a lot of data points.

### Fitting a new kNN model with the optimal parameters found in gridsearch.
"""

best_knn = KNeighborsClassifier(n_neighbors=4,
                                weights='distance',
                                algorithm='ball_tree',
                                p=2)

best_knn.fit(X_train, y_train)

best_knn.score(X_test, y_test)

"""#### Confusion matrix for the optimal kNN model."""

y_pred = best_knn.predict(X_test)

# Compute confusion matrix
cnf_matrix = confusion_matrix(y_test, y_pred)
np.set_printoptions(precision=2)

# Plot non-normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=["Did not survive","Survived"],
                      title='Confusion matrix, without normalisation')

# Plot normalized confusion matrix
plt.figure()
plot_confusion_matrix(cnf_matrix, classes=["Did not survive","Survived"], normalize=True,
                      title='Normalised confusion matrix')

plt.show()

"""#### 7. [BONUS] Plot the ROC curves for the optimized logistic regression model and the optimized kNN model on the same plot."""

plot_roc(y_test, y_pred, "kNN model")